# -*- coding: utf-8 -*-
"""Churn Problem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CLuG-fYZKStcTHf8YKJnPzbua4fWifDf
"""

from sklearn.metrics import roc_auc_score

y_true = [
    0,
    1,
    1,
    0,
    1
]

y_predictions = [
    0.1,
    0.9,
    0.4,
    0.6,
    0.61
]

roc_auc_score(y_true, y_predictions)

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

!gdown 1ERwQ5odiK1Zvi1LtjpkzCMUswYsAX8_K  # train.csv
!gdown 1fGw_-RFwvn_LEdt91Jq-7A-wzG6mmH8r  # test.csv
!gdown 199Mt4OYZNaelT83U-HGDsEYs2YcUGQ6y  # submission.csv

data = pd.read_csv('./train.csv')

# Для вашего удобства списки с именами разных колонок

# Числовые признаки
num_cols = [
    'ClientPeriod',
    'MonthlySpending',
    'TotalSpent'
]

# Категориальные признаки
cat_cols = [
    'Sex',
    'IsSeniorCitizen',
    'HasPartner',
    'HasChild',
    'HasPhoneService',
    'HasMultiplePhoneNumbers',
    'HasInternetService',
    'HasOnlineSecurityService',
    'HasOnlineBackup',
    'HasDeviceProtection',
    'HasTechSupportAccess',
    'HasOnlineTV',
    'HasMovieSubscription',
    'HasContractPhone',
    'IsBillingPaperless',
    'PaymentMethod'
]

feature_cols = num_cols + cat_cols
target_col = 'Churn'

data.info()

data.drop_duplicates()

data.head()

# Дальше проверяю датасет на различные пропуски

data.isnull().sum()

(data == '').sum()

(data == ' ').sum()

# опа нашел

(data == '?').sum()

(data['TotalSpent'] - data['ClientPeriod']*data['MonthlySpending']).sum()

# Сверху проверил утверждение TotalSpent = ClientPeriod * MonthlySpending. 3300 - разница небольшая, значит утверждение верное. 
# Заполним пропуски в TotalSpent 
data.loc[data['TotalSpent'] == ' ', 'TotalSpent'] = data['ClientPeriod']*data['MonthlySpending']
data['TotalSpent'] = pd.to_numeric(data['TotalSpent'])

(data == ' ').sum()

data.loc[data['ClientPeriod'] == 0]

# их клиентами можно не считать!

data['ClientPeriod'].replace(0, np.nan, inplace=True)
data = data.dropna()

(data['ClientPeriod'] == 0).sum()

# 1)
fig, sub = plt.subplots(nrows = len(cat_cols)//3 + 1, ncols = 3, figsize = (15, 15))
x, y, i = 0, 0, 0
while i < len(cat_cols):
  sub[x][y].pie(data[cat_cols[i]].value_counts(), labels = data[cat_cols[i]].unique(), autopct='%1.1f%%')
  sub[x][y].set_title(cat_cols[i])
  i, y = i + 1, y + 1
  if y == 3:
    x, y = x + 1, 0

# 1)
fig, sub = plt.subplots(ncols = len(num_cols), figsize = (20, 5))
bin_for_cols = [5, 10, 500]
for i in range(len(num_cols)):
  sub[i].hist(x = data[num_cols[i]], bins = range(0, int(data[num_cols[i]].max()), bin_for_cols[i]))
  sub[i].set_xlabel(num_cols[i])
  sub[i].set_ylabel('Number of clients')

# 3)
import seaborn as sns
plt.figure(figsize=(10, 10)) 
bins = range(0, int(data['TotalSpent'].max()) + 1000, 1000)
bins_labels = [f'{i}-{i + 999}' for i in range(0, int(data['TotalSpent'].max()), 1000)]
data['TotalSpentBins'] = pd.cut(data['TotalSpent'], bins=bins, labels=bins_labels)
sns.countplot(data=data, x='TotalSpentBins', hue='Churn', alpha=0.7)

# 3)
plt.figure(figsize=(10, 10)) 
bins = range(0, int(data['MonthlySpending'].max()) + 10, 10)
bins_labels = [f'{i}-{i + 9}' for i in range(0, int(data['MonthlySpending'].max()), 10)]
data['MonthlySpendingBins'] = pd.cut(data['MonthlySpending'], bins=bins, labels=bins_labels)
sns.countplot(data=data, x='MonthlySpendingBins', hue='Churn', alpha=0.7)

# 3)
plt.figure(figsize=(10, 10)) 
bins = range(0, int(data['ClientPeriod'].max()) + 5, 5)
bins_labels = [f'{i}-{i + 4}' for i in range(0, int(data['ClientPeriod'].max()), 5)]
data['ClientPeriodBins'] = pd.cut(data['ClientPeriod'], bins=bins, labels=bins_labels)
sns.countplot(data=data, x='ClientPeriodBins', hue='Churn', alpha=0.7)

# 1)
data[target_col].value_counts() # примерно 65:35 - немного несбалансировано, но нармальна

data = data.iloc[:, :-3] # убираю Bins-фичи, которые я использовал для 3)

from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder
from sklearn.pipeline import make_pipeline

df = data.copy()  # data-у менять нельзя, потом еще использовать буду. Вот и делаю копию

df.info()

#X = df.iloc[:, :-1]
y = data['Churn']

sdsc = StandardScaler()
X[num_cols] = sdsc.fit_transform(X[num_cols])
X = pd.get_dummies(X, columns=cat_cols)

X.head()

X.info()

bestScore = 0
LR = LogisticRegression(random_state=42)
param_grid = {'C':[100/(10**(i)) for i in range(6)]}
model = GridSearchCV(LR, param_grid, refit=True, scoring='roc_auc', n_jobs=-1, cv=5, verbose=False)
model.fit(X, y)

model.best_score_

model.best_params_

!pip install catboost

import catboost

X_origin = data.iloc[:, :-1] # вот и та нетронутая дата

X_orig_train, X_orig_valid, y_train, y_valid = train_test_split(X_origin, y, train_size=0.8, random_state=42)

boosting_model3 = catboost.CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, l2_leaf_reg=2, loss_function='Logloss', cat_features=cat_cols, verbose=False)

boosting_model.fit(X_orig_train, y_train)

yt_pred = boosting_model.predict_proba(X_orig_train)[:, 1]
roc_auc_score(y_train, yt_pred)
# score на трейн сете

y_pred = boosting_model.predict_proba(X_orig_valid)[:, 1]
roc_auc_score(y_true=y_valid, y_score=y_pred)
# score на валид сете

from catboost import CatBoostClassifier, Pool, cv

datapool = Pool(data=data.drop(target_col, axis='columns'), label=data[target_col], cat_features=cat_cols)

# Экспериментирую с лернинг рэйтом и кол-вом деревьев
param_grid = {
    "iterations": [100, 300,  500, 1000],
    "learning_rate": [0.03, 0.1, 0.15],
    "depth": [4, 6, 8],
    "l2_leaf_reg": [1, 3],
    "od_type": ['Iter'], 
    "od_wait": [20]
}
gs = boosting_model3.grid_search(param_grid, datapool, cv=5, verbose=False)
print(f"Лучший скор: {gs['params']}")
#print(f"Параметры:{cv_results}")

boosting_model.get_best_score()

"""# Предсказания"""

best_model = madel

X_test = pd.read_csv('./test.csv')
submission = pd.read_csv('./submission.csv')

submission['Churn'] =  boosting_model3.predict_proba(X_test)[:, 1] 
submission.to_csv('./my_submission.csv')